# Synchronous Prediction Endpoint

This endpoint returns the survival probability **immediately** after submitting the input data.

---

## ðŸ”— Endpoint

**POST** `/titanic_sync`

---

## ðŸ§¾ Request Body

```json
{
  "Pclass": 3,
  "Sex": 1,
  "Age": 24,
  "Fare": 15.0
}
```

- `Pclass` â€” Passenger class (1, 2, or 3)
- `Sex` â€” Gender (0 = male, 1 = female)
- `Age` â€” Passenger age
- `Fare` â€” Ticket fare price

---

## âœ… Example Response

```json
{
  "survival_probability": 0.8251
}
```

---

## ðŸ’¡ Notes

- The prediction is computed instantly.
- The model is loaded once when the API starts and reused for all incoming requests.

# Asynchronous Prediction Endpoint

This version handles predictions in the background and lets the user **poll the result** later using a job ID.

---

## ðŸ”— Step 1: Submit a Job

**POST** `/titanic_async`

### ðŸ“¤ Request Body

```json
{
  "Pclass": 1,
  "Sex": 0,
  "Age": 40,
  "Fare": 60.0
}
```

---

### ðŸ“¥ Example Response

```json
{
  "job_id": "job-1712066113000"
}
```

The returned `job_id` is used to check the result.

---

## ðŸ”— Step 2: Retrieve the Result

**GET** `/titanic_async/{job_id}`

### ðŸ”„ Possible Responses:

#### ðŸ”¸ If still processing:
```json
{
  "status": "processing"
}
```

#### âœ… If completed:
```json
{
  "status": "done",
  "survival_probability": 0.8123
}
```

---

## ðŸ’¡ Notes

- Use this endpoint for heavy loads or async pipelines.
- All jobs are stored in memory using a dictionary.
- Each job ID is based on a timestamp (e.g., `job-1712066113000`).
